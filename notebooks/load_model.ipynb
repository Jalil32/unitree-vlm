{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    \"\"\"Check and return the best available device (GPU or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():  # Apple Silicon GPU\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_llama_model(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", quantize=False):\n",
    "    \"\"\"\n",
    "    Load a LLAMA model variant using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        model_name: The model to load from Hugging Face\n",
    "        quantize: Whether to apply 4-bit quantization\n",
    "        \n",
    "    Returns:\n",
    "        model: The loaded model\n",
    "        tokenizer: The tokenizer for the model\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    device = check_gpu()\n",
    "    \n",
    "    # # Load the tokenizer\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set up quantization options if enabled\n",
    "    # if quantize:\n",
    "    #     print(\"Loading with 4-bit quantization...\")\n",
    "    #     model = AutoModelForCausalLM.from_pretrained(\n",
    "    #         model_name,\n",
    "    #         torch_dtype=torch.float16,\n",
    "    #         device_map=\"auto\"\n",
    "    #     )\n",
    "    # else:\n",
    "    #     # Load without quantization\n",
    "    #     model = AutoModelForCausalLM.from_pretrained(\n",
    "    #         model_name,\n",
    "    #         torch_dtype=torch.float16,\n",
    "    #         device_map=\"auto\"\n",
    "    #     )\n",
    "    # model = AutoModel.from_pretrained(\n",
    "    #     model_name\n",
    "    # )\n",
    "    # Load model directly\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "    # Explicitly move model to GPU if available\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded with {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=2056, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded model\n",
    "        tokenizer: The tokenizer for the model\n",
    "        prompt: The input prompt to generate from\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Controls randomness (lower is more deterministic)\n",
    "        \n",
    "    Returns:\n",
    "        The generated text as a string\n",
    "    \"\"\"\n",
    "    # Prepare the model inputs\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model and whether to apply quantization\n",
    "model_name = \"deepseek-ai/Janus-Pro-7B\"\n",
    "use_quantization = False  # Set to True for larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/Janus-Pro-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 19:11:24.917635: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-16 19:11:24.999401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742123485.028833   10471 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742123485.038294   10471 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742123485.108307   10471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742123485.108318   10471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742123485.108320   10471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742123485.108320   10471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-16 19:11:25.115689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 1777.09M parameters\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model(model_name, quantize=use_quantization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: You are an AI bot that writes essays. The following is an essay about visual language machine learning models (1500 word):\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Generated response:\n",
      "You are an AI bot that writes essays. The following is an essay about visual language machine learning models (1500 word): \n",
      "\n",
      "[...]\n",
      "Alright, so I've been thinking about how to apply machine learning models using visual languages. This seems like a fascinating area because it combines two powerful concepts: the ability to represent information in a structured and efficient way (visual languages) and the flexibility of machine learning algorithms.\n",
      "\n",
      "First off, what exactly is a visual language? From my limited understanding, it's a system where data or signals can be encoded into a set of symbols or visual elements that can be decoded by another system. Common examples include icons, shapes, colors, or diagrams. These visual elements have specific meanings and rules for how they can be used together.\n",
      "\n",
      "In this context, integrating machine learning with visual languages would involve training models on datasets represented in visual forms, allowing the models to adapt and learn from these representations. Machine learning techniques, such as neural networks or decision trees, could be applied to process and analyze these visual data types effectively.\n",
      "\n",
      "Another thought: maybe there's a specific application area where this combination is particularly useful. For instance, image recognition systems often use deep learning, which relies heavily on numerical data. But if you replace the numerical data with visual elements (like shapes or color gradients), the model might become more robust, especially when dealing with complex or ambiguous images.\n",
      "\n",
      "Also, considering efficiency, traditional machine learning models might struggle with large volumes of visual data due to their reliance on numerical computations. Visual languages, being inherently parallelizable, could potentially offer a more efficient processing method, reducing computational overhead while maintaining high accuracy.\n",
      "\n",
      "But wait, isn't there a risk here? Converting data into different visual formats might introduce inconsistencies. If the decoding rules aren't clearly defined or if the visual elements don't align well with the underlying data structure, the model's performance could suffer. Therefore, it's crucial to design the visual languages carefully to ensure consistency and correctness.\n",
      "\n",
      "Moreover, challenges related to interpretability also come into play. When converting decisions made by machine learning models into human-readable form, especially through visual means, the clarity and transparency of the reasoning behind each step can be compromised. This could lead to misinterpretations or inefficiencies in the workflow.\n",
      "\n",
      "Let me think about some specific examples where this integration has already proven successful. One example could be speech-to-text systems, where text is converted into spoken words. Here, both visual and machine learning components work together. Another example could be medical imaging analysis, where visual data like X-rays or MRI scans are processed to identify diseases. In such cases, machine learning models trained on visual data can provide accurate diagnoses even when the input data is noisy or ambiguous.\n",
      "\n",
      "Wait, but does that mean that all areas benefit equally from this integration? Or are certain fields more suited than others? For instance, financial data analysis is typically handled numerically, but if we can represent financial metrics visually (like charts or graphs), perhaps enhancing the detection of trends or patterns would be beneficial.\n",
      "\n",
      "I should also consider the potential limitations. As mentioned before, conversion from numerical to visual data might complicate the model, requiring additional processing steps. Additionally, ensuring the visual representation accurately reflects the original data without introducing noise or distortion is critical. It's a delicate balance between capturing the necessary information and maintaining the integrity of the dataset.\n",
      "\n",
      "Furthermore, ethical considerations arise when integrating machine learning with visual languages. Decoding visual data into meaningful insights must be done responsibly, avoiding biased assumptions that could affect fairness in applications like healthcare or finance.\n",
      "\n",
      "So far, my thoughts revolve around the theoretical aspects, but I need to outline a comprehensive approach to integrating machine learning models with visual languages. Maybe starting with defining the visual encoding scheme first. Once the system knows how to encode its data visually, the next step would be designing machine learning models capable of processing those visual inputs effectively.\n",
      "\n",
      "For instance, suppose we're working with images. We could develop a visual language where each pixel is represented by a color symbol (e.g., RGB values). Then, train a convolutional neural network (CNN) to recognize objects based on these color symbols instead of pixel intensities. Similarly, for natural language, create a vocabulary-based visual language mapping tokens to specific icons or shapes, then train a neural network to translate between the icons and the actual meaning.\n",
      "\n",
      "However, this doesn't yet account for the fact that machine learning models are probabilistic and operate on probabilities rather than crisp values. So, the transition from numerical data to visual data would involve handling uncertainty, perhaps through probability distributions over the visual elements.\n",
      "\n",
      "Another aspect is the evaluation of these integrated systems. Traditional metrics like accuracy or F1 score may not capture the effectiveness of a model operating on visual data. Instead, qualitative assessment through user studies or interactive evaluations might be more appropriate, ensuring that the visual outputs align with expected interpretations.\n",
      "\n",
      "Additionally, considering real-world constraints, the integration must handle data size and complexity appropriately. Large-scale systems built using visual languages might require more sophisticated preprocessing or distributed computing to manage the volume of data efficiently.\n",
      "\n",
      "In summary, integrating machine learning models with visual languages involves several key steps:\n",
      "\n",
      "1. Define a consistent visual encoding scheme that captures relevant information and allows for effective communication between the visual domain and machine learning models.\n",
      "2. Design machine learning architectures specifically tailored to process visual data, possibly leveraging techniques that enhance computational efficiency and maintain model accuracy.\n",
      "3. Implement evaluation frameworks that assess the effectiveness of integrated systems, focusing on both quantitative metrics and qualitative feedback mechanisms.\n",
      "4. Address challenges such as decoding ambiguities, ensuring computational efficiency, and mitigating ethical concerns related to bias and interpretation.\n",
      "\n",
      "Now, moving beyond theory, let's delve deeper into each of these points.\n",
      "\n",
      "Firstly, defining a visual encoding scheme is crucial. What defines the \"signal\" that needs to be encoded? For images, pixels; for text, characters; for audio, frequency bands. Each signal must map coherently to the available visual elements. For instance, in speech-to-image synthesis, the waveform is converted into visual elements like lines, shapes, and circles proportional to sound intensity.\n",
      "\n",
      "Secondly, choosing the right machine learning architecture. Depending on the nature of the data and the problem at hand, different approaches may be suitable. For example, in image classification, CNNs are commonly used because they automatically learn hierarchical features. In text-to-image generation, U-Nets or other architectures designed for sequence-to-image tasks might be employed.\n",
      "\n",
      "Thirdly, implementing evaluation methods. Since machine learning operates on probabilities, traditional metrics may not suffice. Metrics like precision, recall, and F1-score can still be adapted, but users will need to evaluate the outcomes qualitatively. Perhaps combining quantitative metrics with user feedback during testing could yield a more holistic assessment.\n",
      "\n",
      "Fourthly, addressing computational efficiency. Processing large datasets with visual languages requires optimization. Techniques like data sampling, progressive validation, and hardware acceleration (like GPUs) can help manage computational demands without compromising model quality.\n",
      "\n",
      "Lastly, ethical considerations. Ensuring that the integration of machine learning and visual languages respects the intent of the original data and avoids unintended biases. For example, in medical imaging, the decoding process must be transparent and unbiased to prevent misdiagnosis.\n",
      "\n",
      "To illustrate these points further, let's take concrete examples.\n",
      "\n",
      "Take the case of object detection in images. Suppose we're using a CNN to detect cars in a dataset where the cars vary widely in size and orientation. A standard CNN might struggle with varying lighting conditions, leading to overfitting or poor generalization. However, by designing a visual encoding system that normalizes or scales the data appropriately, the model can perform better under different conditions.\n",
      "\n",
      "Alternatively, in natural language processing, creating a word-to-symbol mapping using visual elements can help non-native speakers better understand the context. For instance, representing words as icons or diagrams that students can interact with, providing immediate feedback and reinforcing linguistic structures.\n",
      "\n",
      "Moreover, in collaborative projects involving multiple teams, having a shared vision of how data is encoded and how results should be interpreted can mitigate conflicts. Clear documentation and guidelines within the team can facilitate smoother integration processes.\n",
      "\n",
      "In terms of implementation, developing a visual language might start with establishing a standardized notation system. Then, building upon this, the development of machine learning models would require careful alignment between the visual representations and the algorithmic steps.\n",
      "\n",
      "One challenge lies in bridging the gap between symbolic data and continuous signals required by most machine learning algorithms. This necessitates custom preprocessing steps to transform raw data into the visual format suitable for the chosen model.\n",
      "\n",
      "As for the evaluation process, since visual data presents unique challenges, relying solely on quantitative metrics might not capture the true impact of the model. Therefore, incorporating subjective assessments where experts evaluate the generated visual outputs against predefined criteria can provide a more comprehensive understanding of the system's performance.\n",
      "\n",
      "Another important aspect is scalability. Models integrated with visual languages must be able to handle larger datasets or more complex visual representations without significant degradation in performance. Techniques like transfer learning, domain adaptation, and using specialized hardware can help address these scalability issues.\n",
      "\n",
      "Overall, integrating machine learning models with visual languages opens up new possibilities for tackling complex problems across various domains. By leveraging the strengths of both approaches—visual representation's ability to capture structure and semantics, along with machine learning's capacity for learning patterns and making predictions—it becomes possible to build systems that are more robust, interpretable, and capable of handling diverse and challenging scenarios.\n",
      "\n",
      "However, challenges remain in ensuring seamless integration across different visual languages, managing computational resources efficiently, and addressing ethical implications. Overcoming these hurdles will be essential for successfully applying machine learning models in real-world settings where visual languages play a pivotal role.\n",
      "\n",
      "In conclusion, the intersection of machine learning and visual languages offers a promising avenue for advancing both fields. By carefully designing encoding schemes, selecting appropriate models, employing robust evaluation methods, and addressing computational and ethical challenges, it is possible to harness the power of both technologies to develop intelligent systems capable of interpreting, generating, and enhancing visual data.\n",
      "</think>\n",
      "\n",
      "The integration of machine learning models with visual languages presents a transformative approach to solving complex problems across various domains. By combining the structured efficiency of visual languages with the adaptive capabilities of machine learning, this interdisciplinary approach leverages the strengths of both technologies. Below is a detailed exploration of this integration, including key concepts, implementation strategies, and practical considerations.\n",
      "\n",
      "### Key Concepts\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try a simple prompt\n",
    "prompt = \"You are an AI bot that writes essays. The following is an essay about visual language machine learning models (1500 word):\"\n",
    "\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"\\nGenerating response...\")\n",
    "\n",
    "response = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "print(\"\\nGenerated response:\")\n",
    "print(response)\n",
    "\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Select a voice by index (based on previous output)\n",
    "voice_index = 0  # Change this to select different voices\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[voice_index].id)\n",
    "\n",
    "# Set speech rate (optional)\n",
    "engine.setProperty('rate', 150)\n",
    "\n",
    "# Speak text\n",
    "text = \"Hello, this is a custom voice test!\"\n",
    "engine.say(response)\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)  # Should print \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Should print True\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())  # Should be > 0\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())  # For Apple M1/M2 GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.98-py3-none-any.whl.metadata (3.8 kB)\n",
      "Downloading pyttsx3-2.98-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: pyttsx3\n",
      "Successfully installed pyttsx3-2.98\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice 0: Afrikaans - Afrikaans\n",
      "Voice 1: Amharic - Amharic\n",
      "Voice 2: Aragonese - Aragonese\n",
      "Voice 3: Arabic - Arabic\n",
      "Voice 4: Assamese - Assamese\n",
      "Voice 5: Azerbaijani - Azerbaijani\n",
      "Voice 6: Bashkir - Bashkir\n",
      "Voice 7: Belarusian - Belarusian\n",
      "Voice 8: Bulgarian - Bulgarian\n",
      "Voice 9: Bengali - Bengali\n",
      "Voice 10: Bishnupriya Manipuri - Bishnupriya Manipuri\n",
      "Voice 11: Bosnian - Bosnian\n",
      "Voice 12: Catalan - Catalan\n",
      "Voice 13: Cherokee  - Cherokee \n",
      "Voice 14: Chinese (Mandarin, latin as English) - Chinese (Mandarin, latin as English)\n",
      "Voice 15: Chinese (Mandarin, latin as Pinyin) - Chinese (Mandarin, latin as Pinyin)\n",
      "Voice 16: Czech - Czech\n",
      "Voice 17: Chuvash - Chuvash\n",
      "Voice 18: Welsh - Welsh\n",
      "Voice 19: Danish - Danish\n",
      "Voice 20: German - German\n",
      "Voice 21: Greek - Greek\n",
      "Voice 22: English (Caribbean) - English (Caribbean)\n",
      "Voice 23: English (Great Britain) - English (Great Britain)\n",
      "Voice 24: English (Scotland) - English (Scotland)\n",
      "Voice 25: English (Lancaster) - English (Lancaster)\n",
      "Voice 26: English (West Midlands) - English (West Midlands)\n",
      "Voice 27: English (Received Pronunciation) - English (Received Pronunciation)\n",
      "Voice 28: English (America) - English (America)\n",
      "Voice 29: English (America, New York City) - English (America, New York City)\n",
      "Voice 30: Esperanto - Esperanto\n",
      "Voice 31: Spanish (Spain) - Spanish (Spain)\n",
      "Voice 32: Spanish (Latin America) - Spanish (Latin America)\n",
      "Voice 33: Estonian - Estonian\n",
      "Voice 34: Basque - Basque\n",
      "Voice 35: Persian - Persian\n",
      "Voice 36: Persian (Pinglish) - Persian (Pinglish)\n",
      "Voice 37: Finnish - Finnish\n",
      "Voice 38: French (Belgium) - French (Belgium)\n",
      "Voice 39: French (Switzerland) - French (Switzerland)\n",
      "Voice 40: French (France) - French (France)\n",
      "Voice 41: Gaelic (Irish) - Gaelic (Irish)\n",
      "Voice 42: Gaelic (Scottish) - Gaelic (Scottish)\n",
      "Voice 43: Guarani - Guarani\n",
      "Voice 44: Greek (Ancient) - Greek (Ancient)\n",
      "Voice 45: Gujarati - Gujarati\n",
      "Voice 46: Hakka Chinese - Hakka Chinese\n",
      "Voice 47: Hawaiian - Hawaiian\n",
      "Voice 48: Hebrew - Hebrew\n",
      "Voice 49: Hindi - Hindi\n",
      "Voice 50: Croatian - Croatian\n",
      "Voice 51: Haitian Creole - Haitian Creole\n",
      "Voice 52: Hungarian - Hungarian\n",
      "Voice 53: Armenian (East Armenia) - Armenian (East Armenia)\n",
      "Voice 54: Armenian (West Armenia) - Armenian (West Armenia)\n",
      "Voice 55: Interlingua - Interlingua\n",
      "Voice 56: Indonesian - Indonesian\n",
      "Voice 57: Ido - Ido\n",
      "Voice 58: Icelandic - Icelandic\n",
      "Voice 59: Italian - Italian\n",
      "Voice 60: Japanese - Japanese\n",
      "Voice 61: Lojban - Lojban\n",
      "Voice 62: Georgian - Georgian\n",
      "Voice 63: Kazakh - Kazakh\n",
      "Voice 64: Greenlandic - Greenlandic\n",
      "Voice 65: Kannada - Kannada\n",
      "Voice 66: Korean - Korean\n",
      "Voice 67: Konkani - Konkani\n",
      "Voice 68: Kurdish - Kurdish\n",
      "Voice 69: Kyrgyz - Kyrgyz\n",
      "Voice 70: Latin - Latin\n",
      "Voice 71: Luxembourgish - Luxembourgish\n",
      "Voice 72: Lingua Franca Nova - Lingua Franca Nova\n",
      "Voice 73: Lithuanian - Lithuanian\n",
      "Voice 74: Latgalian - Latgalian\n",
      "Voice 75: Latvian - Latvian\n",
      "Voice 76: Māori - Māori\n",
      "Voice 77: Macedonian - Macedonian\n",
      "Voice 78: Malayalam - Malayalam\n",
      "Voice 79: Marathi - Marathi\n",
      "Voice 80: Malay - Malay\n",
      "Voice 81: Maltese - Maltese\n",
      "Voice 82: Myanmar (Burmese) - Myanmar (Burmese)\n",
      "Voice 83: Norwegian Bokmål - Norwegian Bokmål\n",
      "Voice 84: Nahuatl (Classical) - Nahuatl (Classical)\n",
      "Voice 85: Nepali - Nepali\n",
      "Voice 86: Dutch - Dutch\n",
      "Voice 87: Nogai - Nogai\n",
      "Voice 88: Oromo - Oromo\n",
      "Voice 89: Oriya - Oriya\n",
      "Voice 90: Punjabi - Punjabi\n",
      "Voice 91: Papiamento - Papiamento\n",
      "Voice 92: Klingon - Klingon\n",
      "Voice 93: Polish - Polish\n",
      "Voice 94: Portuguese (Portugal) - Portuguese (Portugal)\n",
      "Voice 95: Portuguese (Brazil) - Portuguese (Brazil)\n",
      "Voice 96: Pyash - Pyash\n",
      "Voice 97: Lang_Belta - Lang_Belta\n",
      "Voice 98: Quechua - Quechua\n",
      "Voice 99: K'iche' - K'iche'\n",
      "Voice 100: Quenya - Quenya\n",
      "Voice 101: Romanian - Romanian\n",
      "Voice 102: Russian - Russian\n",
      "Voice 103: Russian (Latvia) - Russian (Latvia)\n",
      "Voice 104: Sindhi - Sindhi\n",
      "Voice 105: Shan (Tai Yai) - Shan (Tai Yai)\n",
      "Voice 106: Sinhala - Sinhala\n",
      "Voice 107: Sindarin - Sindarin\n",
      "Voice 108: Slovak - Slovak\n",
      "Voice 109: Slovenian - Slovenian\n",
      "Voice 110: Lule Saami - Lule Saami\n",
      "Voice 111: Albanian - Albanian\n",
      "Voice 112: Serbian - Serbian\n",
      "Voice 113: Swedish - Swedish\n",
      "Voice 114: Swahili - Swahili\n",
      "Voice 115: Tamil - Tamil\n",
      "Voice 116: Telugu - Telugu\n",
      "Voice 117: Thai - Thai\n",
      "Voice 118: Turkmen - Turkmen\n",
      "Voice 119: Setswana - Setswana\n",
      "Voice 120: Turkish - Turkish\n",
      "Voice 121: Tatar - Tatar\n",
      "Voice 122: Uyghur - Uyghur\n",
      "Voice 123: Ukrainian - Ukrainian\n",
      "Voice 124: Urdu - Urdu\n",
      "Voice 125: Uzbek - Uzbek\n",
      "Voice 126: Vietnamese (Northern) - Vietnamese (Northern)\n",
      "Voice 127: Vietnamese (Central) - Vietnamese (Central)\n",
      "Voice 128: Vietnamese (Southern) - Vietnamese (Southern)\n",
      "Voice 129: Chinese (Cantonese) - Chinese (Cantonese)\n",
      "Voice 130: Chinese (Cantonese, latin as Jyutping) - Chinese (Cantonese, latin as Jyutping)\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Get available voices\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Print available voices\n",
    "for index, voice in enumerate(voices):\n",
    "    print(f\"Voice {index}: {voice.name} - {voice.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Select a voice by index (based on previous output)\n",
    "voice_index = 22  # Change this to select different voices\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[voice_index].id)\n",
    "\n",
    "# Set speech rate (optional)\n",
    "engine.setProperty('rate', 150)\n",
    "\n",
    "# Speak text\n",
    "text = \"Hello, this is a custom voice test!\"\n",
    "engine.say(text)\n",
    "engine.runAndWait()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
